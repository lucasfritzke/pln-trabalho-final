<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="CopilotDiffPersistence">
    <option name="pendingDiffs">
      <map>
        <entry key="$PROJECT_DIR$/notebook_processar_v2.ipynb">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/notebook_processar_v2.ipynb" />
              <option name="originalContent" value="#%% md&#10;#  Processamento e Indexação de Filmes&#10;&#10;Este notebook processa os arquivos de filmes, divide em chunks, gera embeddings e salva no PostgreSQL.&#10;&#10;**Importante:** Verifica duplicatas antes de inserir!&#10;#%% md&#10;## 1. Importações e Configurações&#10;#%%&#10;import os&#10;import re&#10;from pathlib import Path&#10;from typing import List, Tuple&#10;import psycopg2&#10;from psycopg2.extras import execute_values&#10;import numpy as np&#10;from sentence_transformers import SentenceTransformer&#10;from tqdm.notebook import tqdm&#10;import pandas as pd&#10;&#10;print(&quot;✓ Bibliotecas importadas com sucesso&quot;)&#10;#%% md&#10;## 2. Configurações do Sistema&#10;#%%&#10;DB_CONFIG = {&#10;    'dbname': 'filmes_rag',&#10;    'user': 'postgres',&#10;    'password': 'senha123',&#10;    'host': 'localhost',&#10;    'port': '5432'&#10;}&#10;&#10;DIRETORIO_FILMES = 'filmestxts'&#10;MODELO_EMBEDDING = 'sentence-transformers/paraphrase-multilingual-mpnet-base-v2'&#10;TAMANHO_CHUNK = 500&#10;OVERLAP_CHUNK = 100&#10;&#10;print(&quot;✓ Configurações carregadas&quot;)&#10;#%% md&#10;## 3. Funções Auxiliares&#10;#%%&#10;def limpar_texto(texto: str) -&gt; str:&#10;    texto = re.sub(r'\s+', ' ', texto)&#10;    return texto.strip()&#10;&#10;def criar_chunks(texto: str, tamanho: int, overlap: int) -&gt; List[str]:&#10;    chunks = []&#10;    inicio = 0&#10;    while inicio &lt; len(texto):&#10;        fim = inicio + tamanho&#10;        chunk = texto[inicio:fim]&#10;        if len(chunk.strip()) &gt; 50:&#10;            chunks.append(chunk.strip())&#10;        inicio += (tamanho - overlap)&#10;    return chunks&#10;&#10;def extrair_titulo(nome_arquivo: str) -&gt; str:&#10;    return Path(nome_arquivo).stem&#10;&#10;print(&quot;✓ Funções auxiliares carregadas&quot;)&#10;#%% md&#10;## 4. Conectar ao Banco de Dados&#10;#%%&#10;try:&#10;    conn = psycopg2.connect(**DB_CONFIG)&#10;    cursor = conn.cursor()&#10;    print(&quot;✓ Conectado ao PostgreSQL&quot;)&#10;    &#10;    cursor.execute(&quot;SELECT extversion FROM pg_extension WHERE extname = 'vector';&quot;)&#10;    versao = cursor.fetchone()&#10;    if versao:&#10;        print(f&quot;✓ pgVector versão {versao[0]}&quot;)&#10;    else:&#10;        print(&quot;✗ pgVector NÃO encontrada!&quot;)&#10;except Exception as e:&#10;    print(f&quot;✗ Erro: {e}&quot;)&#10;    raise&#10;#%% md&#10;## 5. Funções de Banco de Dados&#10;#%%&#10;def verificar_filme_existe(titulo: str) -&gt; bool:&#10;    cursor.execute(&quot;SELECT COUNT(*) FROM filmes WHERE titulo = %s;&quot;, (titulo,))&#10;    return cursor.fetchone()[0] &gt; 0&#10;&#10;def obter_chunks_existentes(titulo: str) -&gt; set:&#10;    cursor.execute(&quot;SELECT chunk_index FROM filmes WHERE titulo = %s;&quot;, (titulo,))&#10;    return {row[0] for row in cursor.fetchall()}&#10;&#10;def remover_filme(titulo: str):&#10;    cursor.execute(&quot;DELETE FROM filmes WHERE titulo = %s;&quot;, (titulo,))&#10;    conn.commit()&#10;&#10;def inserir_chunks(titulo: str, chunks: List[str], embeddings: np.ndarray, indices_existentes: set = None):&#10;    if indices_existentes is None:&#10;        indices_existentes = set()&#10;    &#10;    dados_novos = [&#10;        (titulo, chunk, idx, embedding.tolist())&#10;        for idx, (chunk, embedding) in enumerate(zip(chunks, embeddings))&#10;        if idx not in indices_existentes&#10;    ]&#10;    &#10;    if not dados_novos:&#10;        return 0&#10;    &#10;    query = &quot;&quot;&quot;INSERT INTO filmes (titulo, chunk_texto, chunk_index, vetor_embedding)&#10;               VALUES %s ON CONFLICT (titulo, chunk_index) DO NOTHING;&quot;&quot;&quot;&#10;    execute_values(cursor, query, dados_novos)&#10;    conn.commit()&#10;    return len(dados_novos)&#10;&#10;def listar_filmes_indexados() -&gt; pd.DataFrame:&#10;    query = &quot;&quot;&quot;SELECT titulo, COUNT(*) as num_chunks, MIN(data_insercao) as data_insercao&#10;               FROM filmes GROUP BY titulo ORDER BY titulo;&quot;&quot;&quot;&#10;    return pd.read_sql_query(query, conn)&#10;&#10;print(&quot;✓ Funções de BD carregadas&quot;)&#10;#%% md&#10;## 6. Carregar Modelo&#10;#%%&#10;print(&quot;Carregando modelo...&quot;)&#10;modelo = SentenceTransformer(MODELO_EMBEDDING)&#10;&#10;if modelo.device.type == 'cuda':&#10;    import torch&#10;    print(f&quot;✓ GPU: {torch.cuda.get_device_name(0)}&quot;)&#10;else:&#10;    print(&quot;✓ CPU (mais lento)&quot;)&#10;&#10;print(f&quot;✓ Dimensão: {modelo.get_sentence_embedding_dimension()}&quot;)&#10;#%% md&#10;## 7. Ver Filmes Indexados&#10;#%%&#10;df_indexados = listar_filmes_indexados()&#10;if len(df_indexados) &gt; 0:&#10;    print(f&quot; {len(df_indexados)} filmes indexados\n&quot;)&#10;    display(df_indexados)&#10;else:&#10;    print(&quot; Nenhum filme indexado&quot;)&#10;#%% md&#10;## 8. Listar Arquivos&#10;#%%&#10;caminho_filmes = Path(DIRETORIO_FILMES)&#10;arquivos_txt = list(caminho_filmes.glob(&quot;*.txt&quot;)) if caminho_filmes.exists() else []&#10;&#10;print(f&quot; {len(arquivos_txt)} arquivos em '{DIRETORIO_FILMES}'\n&quot;)&#10;&#10;if arquivos_txt:&#10;    filmes_info = []&#10;    for arquivo in arquivos_txt:&#10;        titulo = extrair_titulo(arquivo.name)&#10;        existe = verificar_filme_existe(titulo)&#10;        filmes_info.append({&#10;            'Arquivo': arquivo.name,&#10;            'Título': titulo,&#10;            'Tamanho (KB)': f&quot;{arquivo.stat().st_size / 1024:.1f}&quot;,&#10;            'Status': '✓ Indexado' if existe else '⏳ Pendente'&#10;        })&#10;    &#10;    df_arquivos = pd.DataFrame(filmes_info)&#10;    display(df_arquivos)&#10;    &#10;    pendentes = len([f for f in filmes_info if '⏳' in f['Status']])&#10;    print(f&quot;\n{len(arquivos_txt) - pendentes} indexados, {pendentes} pendentes&quot;)&#10;#%% md&#10;## 9. Processar Novos Filmes&#10;#%%&#10;print(&quot; Processando novos filmes...\n&quot;)&#10;&#10;stats = {'processados': 0, 'pulados': 0, 'chunks': 0, 'erros': []}&#10;&#10;for arquivo in tqdm(arquivos_txt, desc=&quot;Processando&quot;):&#10;    try:&#10;        titulo = extrair_titulo(arquivo.name)&#10;        &#10;        if verificar_filme_existe(titulo):&#10;            print(f&quot;⏭️ {titulo}&quot;)&#10;            stats['pulados'] += 1&#10;            continue&#10;        &#10;        print(f&quot;\n {titulo}&quot;)&#10;        &#10;        with open(arquivo, 'r', encoding='utf-8') as f:&#10;            conteudo = f.read()&#10;        &#10;        texto_limpo = limpar_texto(conteudo)&#10;        chunks = criar_chunks(texto_limpo, TAMANHO_CHUNK, OVERLAP_CHUNK)&#10;        print(f&quot;   {len(chunks)} chunks&quot;)&#10;        &#10;        if not chunks:&#10;            continue&#10;        &#10;        embeddings = modelo.encode(chunks, batch_size=32, show_progress_bar=False, convert_to_numpy=True)&#10;        inseridos = inserir_chunks(titulo, chunks, embeddings)&#10;        print(f&quot;   ✓ {inseridos} inseridos&quot;)&#10;        &#10;        stats['processados'] += 1&#10;        stats['chunks'] += inseridos&#10;        &#10;    except Exception as e:&#10;        print(f&quot;   ✗ Erro: {e}&quot;)&#10;        stats['erros'].append(f&quot;{arquivo.name}: {e}&quot;)&#10;&#10;print(&quot;\n&quot; + &quot;=&quot;*60)&#10;print(&quot; RESUMO&quot;)&#10;print(&quot;=&quot;*60)&#10;print(f&quot;Processados: {stats['processados']}&quot;)&#10;print(f&quot;Pulados: {stats['pulados']}&quot;)&#10;print(f&quot;Chunks: {stats['chunks']}&quot;)&#10;print(f&quot;Erros: {len(stats['erros'])}&quot;)&#10;if stats['erros']:&#10;    for erro in stats['erros']:&#10;        print(f&quot;  - {erro}&quot;)&#10;#%% md&#10;## 10. Reprocessar Filme Específico&#10;#%%&#10;ARQUIVO_REPROCESSAR = &quot;007 - Operação Skyfall.txt&quot;&#10;&#10;arquivo_path = caminho_filmes / ARQUIVO_REPROCESSAR&#10;&#10;if arquivo_path.exists():&#10;    titulo = extrair_titulo(ARQUIVO_REPROCESSAR)&#10;    &#10;    if verificar_filme_existe(titulo):&#10;        print(f&quot;️ Removendo '{titulo}'...&quot;)&#10;        remover_filme(titulo)&#10;        print(&quot;✓ Removido\n&quot;)&#10;    &#10;    print(f&quot; Reprocessando: {titulo}\n&quot;)&#10;    &#10;    with open(arquivo_path, 'r', encoding='utf-8') as f:&#10;        conteudo = f.read()&#10;    &#10;    texto_limpo = limpar_texto(conteudo)&#10;    chunks = criar_chunks(texto_limpo, TAMANHO_CHUNK, OVERLAP_CHUNK)&#10;    embeddings = modelo.encode(chunks, show_progress_bar=True, convert_to_numpy=True)&#10;    inseridos = inserir_chunks(titulo, chunks, embeddings)&#10;    &#10;    print(f&quot;\n✓ {inseridos} chunks inseridos&quot;)&#10;else:&#10;    print(f&quot;✗ Arquivo não encontrado&quot;)&#10;#%% md&#10;## 11. Limpar Tudo (CUIDADO!)&#10;#%%&#10;# Descomente para usar:&#10;# resposta = input(&quot;⚠️ APAGAR TUDO? (digite SIM): &quot;)&#10;# if resposta == &quot;SIM&quot;:&#10;#     cursor.execute(&quot;TRUNCATE TABLE filmes RESTART IDENTITY;&quot;)&#10;#     conn.commit()&#10;#     print(&quot;✓ Banco limpo&quot;)&#10;# else:&#10;#     print(&quot;❌ Cancelado&quot;)&#10;&#10;print(&quot;ℹ️ Célula comentada por segurança&quot;)&#10;#%% md&#10;## 12. Estatísticas Finais&#10;#%%&#10;print(&quot;\n ESTATÍSTICAS&quot;)&#10;print(&quot;=&quot;*60)&#10;&#10;cursor.execute(&quot;SELECT COUNT(*) FROM filmes;&quot;)&#10;total_chunks = cursor.fetchone()[0]&#10;print(f&quot;Chunks: {total_chunks}&quot;)&#10;&#10;cursor.execute(&quot;SELECT COUNT(DISTINCT titulo) FROM filmes;&quot;)&#10;total_filmes = cursor.fetchone()[0]&#10;print(f&quot;Filmes: {total_filmes}&quot;)&#10;&#10;if total_filmes &gt; 0:&#10;    print(f&quot;Média: {total_chunks / total_filmes:.1f} chunks/filme&quot;)&#10;&#10;cursor.execute(&quot;SELECT pg_size_pretty(pg_database_size('filmes_rag'));&quot;)&#10;print(f&quot;Tamanho: {cursor.fetchone()[0]}&quot;)&#10;&#10;print(&quot;\n&quot;)&#10;df_final = listar_filmes_indexados()&#10;if len(df_final) &gt; 0:&#10;    display(df_final)&#10;#%% md&#10;## 13. Testar Busca&#10;#%%&#10;def buscar_teste(query: str, top_k: int = 3):&#10;    print(f&quot;\n '{query}'\n&quot;)&#10;    &#10;    emb = modelo.encode([query], convert_to_numpy=True)[0]&#10;    &#10;    sql = &quot;&quot;&quot;SELECT titulo, chunk_texto, 1 - (vetor_embedding &lt;=&gt; %s::vector) as sim&#10;             FROM filmes ORDER BY vetor_embedding &lt;=&gt; %s::vector LIMIT %s;&quot;&quot;&quot;&#10;    &#10;    cursor.execute(sql, (emb.tolist(), emb.tolist(), top_k))&#10;    &#10;    for i, (titulo, chunk, sim) in enumerate(cursor.fetchall(), 1):&#10;        print(f&quot;[{i}] {titulo} ({sim:.1%})&quot;)&#10;        print(f&quot;    {chunk[:120]}...\n&quot;)&#10;&#10;buscar_teste(&quot;filme de gelo&quot;)&#10;buscar_teste(&quot;comédia divertida&quot;)&#10;#%% md&#10;## 14. Fechar Conexão&#10;#%%&#10;cursor.close()&#10;conn.close()&#10;print(&quot;✓ Conexão fechada&quot;)&#10;print(&quot;\n✅ Pronto!&quot;)&#10;print(&quot;   Execute: streamlit run interface_busca.py&quot;)" />
              <option name="updatedContent" value="#%% md&#10;#  Processamento e Indexação de Filmes&#10;&#10;Este notebook processa os arquivos de filmes, divide em chunks, gera embeddings e salva no PostgreSQL.&#10;&#10;**Importante:** Verifica duplicatas antes de inserir!&#10;#%% md&#10;## 1. Importações e Configurações&#10;#%%&#10;import os&#10;import re&#10;from pathlib import Path&#10;from typing import List, Tuple&#10;import psycopg2&#10;from psycopg2.extras import execute_values&#10;import numpy as np&#10;from sentence_transformers import SentenceTransformer&#10;from tqdm.notebook import tqdm&#10;import pandas as pd&#10;&#10;print(&quot;✓ Bibliotecas importadas com sucesso&quot;)&#10;#%% md&#10;## 2. Configurações do Sistema&#10;#%%&#10;DB_CONFIG = {&#10;    'dbname': 'filmes_rag',&#10;    'user': 'postgres',&#10;    'password': 'senha123',&#10;    'host': 'localhost',&#10;    'port': '5432'&#10;}&#10;&#10;DIRETORIO_FILMES = 'filmestxts'&#10;MODELO_EMBEDDING = 'sentence-transformers/paraphrase-multilingual-mpnet-base-v2'&#10;TAMANHO_CHUNK = 500&#10;OVERLAP_CHUNK = 100&#10;&#10;print(&quot;✓ Configurações carregadas&quot;)&#10;#%% md&#10;## 3. Funções Auxiliares&#10;#%%&#10;def limpar_texto(texto: str) -&gt; str:&#10;    texto = re.sub(r'\s+', ' ', texto)&#10;    return texto.strip()&#10;&#10;def criar_chunks(texto: str, tamanho: int, overlap: int) -&gt; List[str]:&#10;    chunks = []&#10;    inicio = 0&#10;    while inicio &lt; len(texto):&#10;        fim = inicio + tamanho&#10;        chunk = texto[inicio:fim]&#10;        if len(chunk.strip()) &gt; 50:&#10;            chunks.append(chunk.strip())&#10;        inicio += (tamanho - overlap)&#10;    return chunks&#10;&#10;def extrair_titulo(nome_arquivo: str) -&gt; str:&#10;    return Path(nome_arquivo).stem&#10;&#10;print(&quot;✓ Funções auxiliares carregadas&quot;)&#10;#%% md&#10;## 4. Conectar ao Banco de Dados&#10;#%%&#10;try:&#10;    conn = psycopg2.connect(**DB_CONFIG)&#10;    cursor = conn.cursor()&#10;    print(&quot;✓ Conectado ao PostgreSQL&quot;)&#10;    &#10;    cursor.execute(&quot;SELECT extversion FROM pg_extension WHERE extname = 'vector';&quot;)&#10;    versao = cursor.fetchone()&#10;    if versao:&#10;        print(f&quot;✓ pgVector versão {versao[0]}&quot;)&#10;    else:&#10;        print(&quot;✗ pgVector NÃO encontrada!&quot;)&#10;except Exception as e:&#10;    print(f&quot;✗ Erro: {e}&quot;)&#10;    raise&#10;#%% md&#10;## 5. Funções de Banco de Dados&#10;#%%&#10;def verificar_filme_existe(titulo: str) -&gt; bool:&#10;    cursor.execute(&quot;SELECT COUNT(*) FROM filmes WHERE titulo = %s;&quot;, (titulo,))&#10;    return cursor.fetchone()[0] &gt; 0&#10;&#10;def obter_chunks_existentes(titulo: str) -&gt; set:&#10;    cursor.execute(&quot;SELECT chunk_index FROM filmes WHERE titulo = %s;&quot;, (titulo,))&#10;    return {row[0] for row in cursor.fetchall()}&#10;&#10;def remover_filme(titulo: str):&#10;    cursor.execute(&quot;DELETE FROM filmes WHERE titulo = %s;&quot;, (titulo,))&#10;    conn.commit()&#10;&#10;def inserir_chunks(titulo: str, chunks: List[str], embeddings: np.ndarray, indices_existentes: set = None):&#10;    if indices_existentes is None:&#10;        indices_existentes = set()&#10;    &#10;    dados_novos = [&#10;        (titulo, chunk, idx, embedding.tolist())&#10;        for idx, (chunk, embedding) in enumerate(zip(chunks, embeddings))&#10;        if idx not in indices_existentes&#10;    ]&#10;    &#10;    if not dados_novos:&#10;        return 0&#10;    &#10;    query = &quot;&quot;&quot;INSERT INTO filmes (titulo, chunk_texto, chunk_index, vetor_embedding)&#10;               VALUES %s ON CONFLICT (titulo, chunk_index) DO NOTHING;&quot;&quot;&quot;&#10;    execute_values(cursor, query, dados_novos)&#10;    conn.commit()&#10;    return len(dados_novos)&#10;&#10;def listar_filmes_indexados() -&gt; pd.DataFrame:&#10;    query = &quot;&quot;&quot;SELECT titulo, COUNT(*) as num_chunks, MIN(data_insercao) as data_insercao&#10;               FROM filmes GROUP BY titulo ORDER BY titulo;&quot;&quot;&quot;&#10;    return pd.read_sql_query(query, conn)&#10;&#10;print(&quot;✓ Funções de BD carregadas&quot;)&#10;#%% md&#10;## 6. Carregar Modelo&#10;#%%&#10;print(&quot;Carregando modelo...&quot;)&#10;modelo = SentenceTransformer(MODELO_EMBEDDING)&#10;&#10;if modelo.device.type == 'cuda':&#10;    import torch&#10;    print(f&quot;✓ GPU: {torch.cuda.get_device_name(0)}&quot;)&#10;else:&#10;    print(&quot;✓ CPU (mais lento)&quot;)&#10;&#10;print(f&quot;✓ Dimensão: {modelo.get_sentence_embedding_dimension()}&quot;)&#10;#%% md&#10;## 7. Ver Filmes Indexados&#10;#%%&#10;df_indexados = listar_filmes_indexados()&#10;if len(df_indexados) &gt; 0:&#10;    print(f&quot; {len(df_indexados)} filmes indexados\n&quot;)&#10;    display(df_indexados)&#10;else:&#10;    print(&quot; Nenhum filme indexado&quot;)&#10;#%% md&#10;## 8. Listar Arquivos&#10;#%%&#10;caminho_filmes = Path(DIRETORIO_FILMES)&#10;arquivos_txt = list(caminho_filmes.glob(&quot;*.txt&quot;)) if caminho_filmes.exists() else []&#10;&#10;print(f&quot; {len(arquivos_txt)} arquivos em '{DIRETORIO_FILMES}'\n&quot;)&#10;&#10;if arquivos_txt:&#10;    filmes_info = []&#10;    for arquivo in arquivos_txt:&#10;        titulo = extrair_titulo(arquivo.name)&#10;        existe = verificar_filme_existe(titulo)&#10;        filmes_info.append({&#10;            'Arquivo': arquivo.name,&#10;            'Título': titulo,&#10;            'Tamanho (KB)': f&quot;{arquivo.stat().st_size / 1024:.1f}&quot;,&#10;            'Status': '✓ Indexado' if existe else '⏳ Pendente'&#10;        })&#10;    &#10;    df_arquivos = pd.DataFrame(filmes_info)&#10;    display(df_arquivos)&#10;    &#10;    pendentes = len([f for f in filmes_info if '⏳' in f['Status']])&#10;    print(f&quot;\n{len(arquivos_txt) - pendentes} indexados, {pendentes} pendentes&quot;)&#10;#%% md&#10;## 9. Processar Novos Filmes&#10;#%%&#10;print(&quot; Processando novos filmes...\n&quot;)&#10;&#10;stats = {'processados': 0, 'pulados': 0, 'chunks': 0, 'erros': []}&#10;&#10;for arquivo in tqdm(arquivos_txt, desc=&quot;Processando&quot;):&#10;    try:&#10;        titulo = extrair_titulo(arquivo.name)&#10;        &#10;        if verificar_filme_existe(titulo):&#10;            print(f&quot;⏭️ {titulo}&quot;)&#10;            stats['pulados'] += 1&#10;            continue&#10;        &#10;        print(f&quot;\n {titulo}&quot;)&#10;        &#10;        with open(arquivo, 'r', encoding='utf-8') as f:&#10;            conteudo = f.read()&#10;        &#10;        texto_limpo = limpar_texto(conteudo)&#10;        chunks = criar_chunks(texto_limpo, TAMANHO_CHUNK, OVERLAP_CHUNK)&#10;        print(f&quot;   {len(chunks)} chunks&quot;)&#10;        &#10;        if not chunks:&#10;            continue&#10;        &#10;        embeddings = modelo.encode(chunks, batch_size=32, show_progress_bar=False, convert_to_numpy=True)&#10;        inseridos = inserir_chunks(titulo, chunks, embeddings)&#10;        print(f&quot;   ✓ {inseridos} inseridos&quot;)&#10;        &#10;        stats['processados'] += 1&#10;        stats['chunks'] += inseridos&#10;        &#10;    except Exception as e:&#10;        print(f&quot;   ✗ Erro: {e}&quot;)&#10;        stats['erros'].append(f&quot;{arquivo.name}: {e}&quot;)&#10;&#10;print(&quot;\n&quot; + &quot;=&quot;*60)&#10;print(&quot; RESUMO&quot;)&#10;print(&quot;=&quot;*60)&#10;print(f&quot;Processados: {stats['processados']}&quot;)&#10;print(f&quot;Pulados: {stats['pulados']}&quot;)&#10;print(f&quot;Chunks: {stats['chunks']}&quot;)&#10;print(f&quot;Erros: {len(stats['erros'])}&quot;)&#10;if stats['erros']:&#10;    for erro in stats['erros']:&#10;        print(f&quot;  - {erro}&quot;)&#10;#%% md&#10;## 10. Reprocessar Filme Específico&#10;#%%&#10;ARQUIVO_REPROCESSAR = &quot;007 - Operação Skyfall.txt&quot;&#10;&#10;arquivo_path = caminho_filmes / ARQUIVO_REPROCESSAR&#10;&#10;if arquivo_path.exists():&#10;    titulo = extrair_titulo(ARQUIVO_REPROCESSAR)&#10;    &#10;    if verificar_filme_existe(titulo):&#10;        print(f&quot;️ Removendo '{titulo}'...&quot;)&#10;        remover_filme(titulo)&#10;        print(&quot;✓ Removido\n&quot;)&#10;    &#10;    print(f&quot; Reprocessando: {titulo}\n&quot;)&#10;    &#10;    with open(arquivo_path, 'r', encoding='utf-8') as f:&#10;        conteudo = f.read()&#10;    &#10;    texto_limpo = limpar_texto(conteudo)&#10;    chunks = criar_chunks(texto_limpo, TAMANHO_CHUNK, OVERLAP_CHUNK)&#10;    embeddings = modelo.encode(chunks, show_progress_bar=True, convert_to_numpy=True)&#10;    inseridos = inserir_chunks(titulo, chunks, embeddings)&#10;    &#10;    print(f&quot;\n✓ {inseridos} chunks inseridos&quot;)&#10;else:&#10;    print(f&quot;✗ Arquivo não encontrado&quot;)&#10;#%% md&#10;## 11. Limpar Tudo (CUIDADO!)&#10;#%%&#10;# Descomente para usar:&#10;# resposta = input(&quot;⚠️ APAGAR TUDO? (digite SIM): &quot;)&#10;# if resposta == &quot;SIM&quot;:&#10;#     cursor.execute(&quot;TRUNCATE TABLE filmes RESTART IDENTITY;&quot;)&#10;#     conn.commit()&#10;#     print(&quot;✓ Banco limpo&quot;)&#10;# else:&#10;#     print(&quot;❌ Cancelado&quot;)&#10;&#10;print(&quot;ℹ️ Célula comentada por segurança&quot;)&#10;#%% md&#10;## 12. Estatísticas Finais&#10;#%%&#10;print(&quot;\n ESTATÍSTICAS&quot;)&#10;print(&quot;=&quot;*60)&#10;&#10;cursor.execute(&quot;SELECT COUNT(*) FROM filmes;&quot;)&#10;total_chunks = cursor.fetchone()[0]&#10;print(f&quot;Chunks: {total_chunks}&quot;)&#10;&#10;cursor.execute(&quot;SELECT COUNT(DISTINCT titulo) FROM filmes;&quot;)&#10;total_filmes = cursor.fetchone()[0]&#10;print(f&quot;Filmes: {total_filmes}&quot;)&#10;&#10;if total_filmes &gt; 0:&#10;    print(f&quot;Média: {total_chunks / total_filmes:.1f} chunks/filme&quot;)&#10;&#10;cursor.execute(&quot;SELECT pg_size_pretty(pg_database_size('filmes_rag'));&quot;)&#10;print(f&quot;Tamanho: {cursor.fetchone()[0]}&quot;)&#10;&#10;print(&quot;\n&quot;)&#10;df_final = listar_filmes_indexados()&#10;if len(df_final) &gt; 0:&#10;    display(df_final)&#10;#%% md&#10;## 13. Testar Busca&#10;#%%&#10;def buscar_teste(query: str, top_k: int = 3):&#10;    print(f&quot;\n '{query}'\n&quot;)&#10;    &#10;    emb = modelo.encode([query], convert_to_numpy=True)[0]&#10;    &#10;    sql = &quot;&quot;&quot;SELECT titulo, chunk_texto, 1 - (vetor_embedding &lt;=&gt; %s::vector) as sim&#10;             FROM filmes ORDER BY vetor_embedding &lt;=&gt; %s::vector LIMIT %s;&quot;&quot;&quot;&#10;    &#10;    cursor.execute(sql, (emb.tolist(), emb.tolist(), top_k))&#10;    &#10;    for i, (titulo, chunk, sim) in enumerate(cursor.fetchall(), 1):&#10;        print(f&quot;[{i}] {titulo} ({sim:.1%})&quot;)&#10;        print(f&quot;    {chunk[:120]}...\n&quot;)&#10;&#10;buscar_teste(&quot;filme de gelo&quot;)&#10;buscar_teste(&quot;comédia divertida&quot;)&#10;#%% md&#10;## 14. Fechar Conexão&#10;#%%&#10;cursor.close()&#10;conn.close()&#10;print(&quot;✓ Conexão fechada&quot;)&#10;print(&quot;\n✅ Pronto!&quot;)&#10;print(&quot;   Execute: streamlit run interface_busca.py&quot;)" />
            </PendingDiffInfo>
          </value>
        </entry>
      </map>
    </option>
  </component>
</project>